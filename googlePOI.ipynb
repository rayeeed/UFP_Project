{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeIt2gKfm9TbHzlG4XD0PY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rayeeed/UFP_Project/blob/main/googlePOI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eN7mtXOYYsQc",
        "outputId": "9cb8c32c-8dfd-440a-e1fc-41a580bbdd25"
      },
      "source": [
        "!pip install unicodecsv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unicodecsv\n",
            "  Downloading unicodecsv-0.14.1.tar.gz (10 kB)\n",
            "Building wheels for collected packages: unicodecsv\n",
            "  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-py3-none-any.whl size=10765 sha256=411f19783b50f4a31f735cf08ebe84bf046a865d9119ff463e646c4ecd78dd4f\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/f4/8a/a5024fb77b32ed369e5c409081e5f00fbe3b92fdad653f6e69\n",
            "Successfully built unicodecsv\n",
            "Installing collected packages: unicodecsv\n",
            "Successfully installed unicodecsv-0.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdhaopP7YxfB"
      },
      "source": [
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO5Ra-EVYUAk"
      },
      "source": [
        "import unicodecsv as csv\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "import numbers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNjErTwBaWiz"
      },
      "source": [
        "# inputs parameters\n",
        "lat_lon_file = \"/content/output_count_289.csv\" # the file that contain centroids points\n",
        "google_places_types = \"/content/google_places_types.csv\" # the file that contain all google places types\n",
        "api_key = \"\" # insert your google api key\n",
        "\n",
        "search_radius = 1000 #METER\n",
        "counter = 0\n",
        "querylimit = 1000\n",
        "\n",
        "xy = pd.read_csv(lat_lon_file)\n",
        "cleaned_xy = xy.loc[(xy['latitude']>=1) & (xy['longitude']<0),:]\n",
        "\n",
        "google_types = pd.read_csv(google_places_types, header = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "XitWRDQkfszy",
        "outputId": "9ff5d691-cc9e-4eec-e0ae-b377e6e93b68"
      },
      "source": [
        "google_types.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>laundry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>painter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>car repair</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gas station</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>restaurants</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             0\n",
              "0      laundry\n",
              "1      painter\n",
              "2   car repair\n",
              "3  gas station\n",
              "4  restaurants"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6vOpbUUcVJz"
      },
      "source": [
        "def get_attribute_value(place, attribute):\n",
        "    if attribute in place:\n",
        "        return place[attribute]\n",
        "    else:\n",
        "        return \"N/A\"\n",
        "\n",
        "def clean_json(place, objectid):\n",
        "    place_details = [objectid,place['place_id'],place['name'], place['reference'], place['geometry']['location']['lat'],\n",
        "                     place['geometry']['location']['lng'], get_attribute_value(place,\"vicinity\"),\n",
        "                     get_attribute_value(place,\"price_level\"), get_attribute_value(place,\"rating\")]\n",
        "    return place_details"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaAq9Wr9ceV9"
      },
      "source": [
        "def find_google_places(lat, lon, search_radius, search_type, api_key, objectid):\n",
        "    global token_list\n",
        "    global counter\n",
        "    #construct url for python to open\n",
        "    location = str(lat)+\",\"+str(lon)\n",
        "    search_radius = search_radius\n",
        "    if counter < querylimit:\n",
        "        myurl = 'https://maps.googleapis.com/maps/api/place/nearbysearch/json?location=%s&radius=%s&types=%s&key=%s' % (location, search_radius, search_type, api_key)\n",
        "        response = urllib2.urlopen(myurl)\n",
        "        counter += 1\n",
        "        html = response.read()\n",
        "        places = json.loads(html)\n",
        "        response.close()\n",
        "\n",
        "        places_details = []\n",
        "        next_page_token = \"\"\n",
        "\n",
        "        if places['status'] == 'OK':\n",
        "            for place in places[\"results\"]:\n",
        "                place_details = clean_json(place,objectid)\n",
        "                places_details.append(place_details)\n",
        "\n",
        "        if \"next_page_token\" in places:\n",
        "            next_page_token = places['next_page_token']\n",
        "            token_list.append([objectid, next_page_token])\n",
        "    else:\n",
        "        return \"over limit\"\n",
        "\n",
        "\n",
        "    return places_details # in the format of json!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvRJXLlkcl8f"
      },
      "source": [
        "def find_google_places_nextpage(token, api_key, objectid):\n",
        "    global token_list\n",
        "    global counter\n",
        "    if counter < querylimit:\n",
        "        myurl = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json?pagetoken=%s&key=%s\"%(token,api_key)\n",
        "        result = urllib2.urlopen(myurl)\n",
        "        counter += 1\n",
        "        content = result.read()\n",
        "        places = json.loads(content)\n",
        "        places_details = []\n",
        "        next_page_token = \"\"\n",
        "\n",
        "        if places['status'] == 'OK':\n",
        "            for place in places[\"results\"]:\n",
        "                place_details = clean_json(place,objectid)\n",
        "                places_details.append(place_details)\n",
        "\n",
        "        if \"next_page_token\" in places:\n",
        "            next_page_token = places['next_page_token']\n",
        "            token_list.append([objectid, next_page_token])\n",
        "        return places_details\n",
        "    else:\n",
        "        return \"over limit\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w01a-4lgjSJ"
      },
      "source": [
        "from io import BytesIO\n",
        "import codecs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvIqnlDMe47J"
      },
      "source": [
        "class UnicodeWriter:\n",
        "    \"\"\"\n",
        "    A CSV writer which will write rows to CSV file \"f\",\n",
        "    which is encoded in the given encoding.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, f, dialect=csv.excel, encoding=\"utf-8\", **kwds):\n",
        "        # Redirect output to a queue\n",
        "        self.queue = BytesIO()\n",
        "        self.writer = csv.writer(self.queue, dialect=dialect, **kwds)\n",
        "        self.stream = f\n",
        "        self.encoder = codecs.getincrementalencoder(encoding)()\n",
        "\n",
        "    def writerow(self, row):\n",
        "        self.writer.writerow([s.encode(\"utf-8\") for s in row])\n",
        "        # Fetch UTF-8 output from the queue ...\n",
        "        data = self.queue.getvalue()\n",
        "        data = data.decode(\"utf-8\")\n",
        "        # ... and reencode it into the target encoding\n",
        "        data = self.encoder.encode(data)\n",
        "        # write to the target stream\n",
        "        self.stream.write(data)\n",
        "        # empty queue\n",
        "        self.queue.truncate(0)\n",
        "\n",
        "    def writerows(self, rows):\n",
        "        for row in rows:\n",
        "            self.writerow(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3A3wtG9cqfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b54be81-ce44-4a73-81cc-2680534481e4"
      },
      "source": [
        "for i in range(0,len(google_types)):  # for each google place type\n",
        "    search_type = google_types.iloc[i, 0]\n",
        "    cleaned_xy[search_type] = 0\n",
        "    token_list = []\n",
        "\n",
        "    output_file = \"/content/ places_%s.csv\" % search_type \n",
        "    with open(output_file,\"wb\") as outputfile:\n",
        "        csvwriter = UnicodeWriter(outputfile)\n",
        "        csvwriter.writerow(['OBJECTID','place_id','name', 'reference', 'lat','lng','vicinity',   \n",
        "                         \"price_level\",\"rating\"]) #restaurant, gas station, market, bus stop\n",
        "        print (\"start searching for %s\" %search_type)\n",
        "        for index, row in cleaned_xy.iterrows():\n",
        "            print (index, counter)\n",
        "            lat = row[\"latitude\"]\n",
        "            lon = row[\"longitude\"]\n",
        "            objectid = index\n",
        "            try:\n",
        "                places = find_google_places(lat, lon, search_radius, search_type, api_key, objectid)\n",
        "                if places == \"over limit\":\n",
        "                    print (\"over limit\",search_type, objectid)\n",
        "                    break\n",
        "            except:\n",
        "                time.sleep(2)\n",
        "                places = find_google_places(lat, lon, search_radius, search_type, api_key, objectid)\n",
        "\n",
        "            cleaned_xy.loc[index, search_type] = len(places)\n",
        "            # write places to csv\n",
        "            res = []\n",
        "            for row in places:\n",
        "                res.append([str(s) if isinstance(s, numbers.Number) else s for s in row])\n",
        "            csvwriter.writerows(res)\n",
        "\n",
        "        time.sleep(2)\n",
        "        while len(token_list)>0:\n",
        "            [objectid, token] = token_list.pop(0)\n",
        "            print (objectid, len(token_list))\n",
        "            try:\n",
        "                places = find_google_places_nextpage(token, api_key, objectid)\n",
        "                if places == \"over limit\":\n",
        "                    print (\"over limit\",search_type, objectid)\n",
        "                    break\n",
        "            except:\n",
        "                time.sleep(2)\n",
        "                places = find_google_places_nextpage(token, api_key, objectid)\n",
        "            cleaned_xy.loc[objectid, search_type] += len(places)\n",
        "            res = []\n",
        "            for row in places:\n",
        "                res.append([str(s) if isinstance(s, numbers.Number) else s for s in row])\n",
        "            csvwriter.writerows(res)\n",
        "\n",
        "    cleaned_xy.to_csv(lat_lon_file)\n",
        "    print (\"%s.csv saved\" %search_type)\n",
        "    print ('-'*80)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start searching for laundry\n",
            "laundry.csv saved\n",
            "--------------------------------------------------------------------------------\n",
            "start searching for painter\n",
            "painter.csv saved\n",
            "--------------------------------------------------------------------------------\n",
            "start searching for car repair\n",
            "car repair.csv saved\n",
            "--------------------------------------------------------------------------------\n",
            "start searching for gas station\n",
            "gas station.csv saved\n",
            "--------------------------------------------------------------------------------\n",
            "start searching for restaurants\n",
            "restaurants.csv saved\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}